{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aab7c48",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aariziqbal/Desktop/Homework/ECS_174/project/CVFinalProject/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4367a64d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 1. Load MiDaS Model for Depth Estimation\n",
    "def load_midas_model():\n",
    "    \"\"\"\n",
    "    Loads the MiDaS model for depth estimation.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): The loaded MiDaS model.\n",
    "        transform (torchvision.transforms.Compose): The transformation applied to input images.\n",
    "    \"\"\"\n",
    "    model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "    model.eval()\n",
    "    transform = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").small_transform\n",
    "    return model, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f147381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class FoodSegDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        # Binarize the mask\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed6a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet is a convolutional neural network architecture for image segmentation.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels. Default is 3.\n",
    "        out_channels (int): Number of output channels. Default is 1.\n",
    "    \n",
    "    Attributes:\n",
    "        encoder (nn.Module): Encoder part of the UNet model.\n",
    "        decoder (nn.Module): Decoder part of the UNet model.\n",
    "    \n",
    "    Methods:\n",
    "        forward(x): Forward pass of the UNet model.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = timm.create_model(\"resnet34\", features_only=True, pretrained=True)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the UNet model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n",
    "        \"\"\"\n",
    "        enc_outs = self.encoder(x)\n",
    "        out = self.decoder(enc_outs[-1])\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6e8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unet_model():\n",
    "    \"\"\"\n",
    "    Load the pre-trained UNet model for depth estimation.\n",
    "\n",
    "    Returns:\n",
    "        model (UNet): The loaded UNet model.\n",
    "    \"\"\"\n",
    "    model = UNet()\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(\"path_to_pretrained_unet.pth\", map_location='cpu'))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pre-trained U-Net model not found. Using untrained model.\")\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57299e78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def estimate_depth(model, transform, image):\n",
    "    \"\"\"\n",
    "    Estimates the depth of an image using a given model and transformation.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The depth estimation model.\n",
    "    transform (torchvision.transforms.Compose): The transformation to be applied to the image.\n",
    "    image (PIL.Image.Image): The input image.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The estimated depth map.\n",
    "    \"\"\"\n",
    "    input_batch = transform(image)  # Add batch dimension\n",
    "    print(\"Input Batch Shape\",input_batch.shape)\n",
    "    with torch.no_grad():\n",
    "        depth = model(input_batch)\n",
    "        print(\"Depth Shape\",depth.shape)\n",
    "        # Fix: Ensure depth has the correct shape [batch_size, channels, height, width]\n",
    "        if len(depth.shape) == 5:\n",
    "            \n",
    "            depth = depth.squeeze(1)  # Remove the extra dimension\n",
    "            \n",
    "        depth = depth.squeeze().cpu().numpy()\n",
    "        depth = cv2.resize(depth, (image.shape[1], image.shape[0]))\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d268eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_food(unet_model, image):\n",
    "    print(\"Image Shape\",image.shape)\n",
    "    \"\"\"\n",
    "    Segment food in an image using a U-Net model.\n",
    "\n",
    "    Parameters:\n",
    "    - unet_model (torch.nn.Module): The U-Net model used for segmentation.\n",
    "    - image (PIL.Image.Image): The input image to be segmented.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The binary mask indicating the segmented food regions.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((256, 256)),\n",
    "    ])\n",
    "    input_tensor = transform(image)\n",
    "    \n",
    "    print(\"Input Tensor Shape\",input_tensor.shape)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        mask = unet_model(input_tensor)\n",
    "        mask = mask.squeeze().cpu().numpy()\n",
    "        mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
    "        return mask > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee0a93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def gaussian_edge_blur(image, kernel_size=5, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Apply Gaussian blur and edge enhancement on a NumPy array image.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Input image as a NumPy array (H x W or H x W x C).\n",
    "        kernel_size (int): Size of the Gaussian kernel.\n",
    "        sigma (float): Standard deviation for Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Processed image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Ensure the input image is a 3D tensor (C x H x W)\n",
    "    if image.ndim == 2:  # Grayscale image\n",
    "        image = image[np.newaxis, ...]  # Add channel dimension\n",
    "    elif image.ndim == 3:  # Color image\n",
    "        image = image.transpose(2, 0, 1)  # Convert H x W x C to C x H x W\n",
    "    else:\n",
    "        raise ValueError(\"Input image must be 2D (H x W) or 3D (H x W x C).\")\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    image_tensor = torch.from_numpy(image).float().unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Apply Gaussian Blur\n",
    "    blurred_image = F.gaussian_blur(image_tensor, kernel_size=(kernel_size, kernel_size), sigma=[sigma, sigma])\n",
    "\n",
    "    # Define Sobel operators\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
    "    sobel_y = sobel_x.T\n",
    "\n",
    "    sobel_x = sobel_x.unsqueeze(0).unsqueeze(0)  # Shape (1, 1, 3, 3)\n",
    "    sobel_y = sobel_y.unsqueeze(0).unsqueeze(0)  # Shape (1, 1, 3, 3)\n",
    "\n",
    "    # Apply Sobel operators for edge detection\n",
    "    edges_x = F.conv2d(blurred_image, sobel_x, padding=1)\n",
    "    edges_y = F.conv2d(blurred_image, sobel_y, padding=1)\n",
    "    edges = torch.sqrt(edges_x**2 + edges_y**2)\n",
    "\n",
    "    # Combine blurred image and edges\n",
    "    result = blurred_image * (1 - edges)\n",
    "\n",
    "    # Convert back to NumPy array\n",
    "    result = result.squeeze(0).numpy()  # Remove batch dimension\n",
    "    if result.shape[0] > 1:  # Color image\n",
    "        result = result.transpose(1, 2, 0)  # Convert C x H x W to H x W x C\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1df2bf7c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_volume(depth_map, mask, pixel_area=0.1):\n",
    "    volume = np.sum(depth_map[mask]) * pixel_area\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "787d7c73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to perform depth estimation and volume calculation for a given image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The path to the input image file.\n",
    "    \"\"\"\n",
    "    # Load Image\n",
    "    image_path = 'colorful_plate.jpg'\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    blurred_image = gaussian_edge_blur(image, kernel_size=5, sigma=1.0)\n",
    "    midas_model, midas_transform = load_midas_model()\n",
    "    unet_model = load_unet_model()\n",
    "    \n",
    "    food_mask = segment_food(unet_model, image)\n",
    "    \n",
    "    depth_map = estimate_depth(midas_model, midas_transform, blurred_image)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Segmented Food\")\n",
    "    plt.imshow(food_mask, cmap='gray')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Depth Map\")\n",
    "    plt.imshow(depth_map, cmap='viridis')\n",
    "    plt.show()\n",
    "    \n",
    "    volume = calculate_volume(depth_map, food_mask)\n",
    "    print(f\"Estimated Volume: {volume:.2f} cubic units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b5ff065",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'gaussian_blur'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 12\u001b[0m blurred_image \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_edge_blur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m midas_model, midas_transform \u001b[38;5;241m=\u001b[39m load_midas_model()\n\u001b[1;32m     14\u001b[0m unet_model \u001b[38;5;241m=\u001b[39m load_unet_model()\n",
      "Cell \u001b[0;32mIn[20], line 33\u001b[0m, in \u001b[0;36mgaussian_edge_blur\u001b[0;34m(image, kernel_size, sigma)\u001b[0m\n\u001b[1;32m     30\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(image)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Apply Gaussian Blur\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m blurred_image \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussian_blur\u001b[49m(image_tensor, kernel_size\u001b[38;5;241m=\u001b[39m(kernel_size, kernel_size), sigma\u001b[38;5;241m=\u001b[39m[sigma, sigma])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Define Sobel operators\u001b[39;00m\n\u001b[1;32m     36\u001b[0m sobel_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.functional' has no attribute 'gaussian_blur'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e722a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
