{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training a model to segment an image into 3 classes: background, foreground, and boundary\n",
    "### The model is a U-Net with a ResNet34 encoder\n",
    "### The model is trained on the ISBI 2012 dataset\n",
    "### The model is trained using the Dice loss function\n",
    "### The model is trained using the Adam optimizer\n",
    "### The model is trained using a learning rate scheduler\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from unet import UNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.encoder1 = conv_block(in_channels, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.encoder4 = conv_block(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(1024, 512)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "\n",
    "        self.conv_final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        e4 = self.encoder4(self.pool(e3))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.upconv4(b)\n",
    "        d4 = torch.cat((d4, e4), dim=1)\n",
    "        d4 = self.decoder4(d4)\n",
    "\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, e3), dim=1)\n",
    "        d3 = self.decoder3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, e2), dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat((d1, e1), dim=1)\n",
    "        d1 = self.decoder1(d1)\n",
    "\n",
    "        return torch.sigmoid(self.conv_final(d1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Update the Dataset class to properly handle image and mask processing\n",
    "from torch.utils.data import Dataset\n",
    "class ISBI2012Dataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        for filename in os.listdir(root):\n",
    "            if filename.endswith(\"train-volume.tif\"):\n",
    "                self.images.append(filename)\n",
    "            elif filename.endswith(\"train-labels.tif\"):\n",
    "                self.masks.append(filename)\n",
    "        self.images.sort()\n",
    "        self.masks.sort()\n",
    "        \n",
    "        # Define separate transforms for images and masks\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root, self.images[idx])\n",
    "        mask_path = os.path.join(self.root, self.masks[idx])\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Convert mask to grayscale\n",
    "        \n",
    "        image = self.image_transform(image)\n",
    "        mask = self.mask_transform(mask)\n",
    "        \n",
    "        # Ensure mask is binary\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmenter:\n",
    "    def __init__(self, model, loss_fn, optimizer, device, scheduler=None):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        images, masks = batch\n",
    "        images = images.to(self.device)\n",
    "        masks = masks.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        predictions = self.model(images)\n",
    "        loss = self.loss_fn(predictions, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def eval_step(self, batch):\n",
    "        images, masks = batch\n",
    "        images = images.to(self.device)\n",
    "        masks = masks.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(images)\n",
    "            loss = self.loss_fn(predictions, masks)\n",
    "            \n",
    "        return loss.item(), predictions\n",
    "\n",
    "    def fit(self, train_loader, val_loader, epochs):\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_losses = []\n",
    "            for batch in train_loader:\n",
    "                loss = self.train_step(batch)\n",
    "                train_losses.append(loss)\n",
    "            \n",
    "            # Validation\n",
    "            val_losses = []\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    loss, _ = self.eval_step(batch)\n",
    "                    val_losses.append(loss)\n",
    "            \n",
    "            avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 0.8439\n",
      "Val Loss: 0.7258\n",
      "------------------------------\n",
      "Epoch 2/50\n",
      "Train Loss: 0.7907\n",
      "Val Loss: 0.7263\n",
      "------------------------------\n",
      "Epoch 3/50\n",
      "Train Loss: 0.7539\n",
      "Val Loss: 0.7269\n",
      "------------------------------\n",
      "Epoch 4/50\n",
      "Train Loss: 0.7256\n",
      "Val Loss: 0.7272\n",
      "------------------------------\n",
      "Epoch 5/50\n",
      "Train Loss: 0.7022\n",
      "Val Loss: 0.7269\n",
      "------------------------------\n",
      "Epoch 6/50\n",
      "Train Loss: 0.6820\n",
      "Val Loss: 0.7263\n",
      "------------------------------\n",
      "Epoch 7/50\n",
      "Train Loss: 0.6799\n",
      "Val Loss: 0.7254\n",
      "------------------------------\n",
      "Epoch 8/50\n",
      "Train Loss: 0.6778\n",
      "Val Loss: 0.7241\n",
      "------------------------------\n",
      "Epoch 9/50\n",
      "Train Loss: 0.6755\n",
      "Val Loss: 0.7224\n",
      "------------------------------\n",
      "Epoch 10/50\n",
      "Train Loss: 0.6732\n",
      "Val Loss: 0.7204\n",
      "------------------------------\n",
      "Epoch 11/50\n",
      "Train Loss: 0.6709\n",
      "Val Loss: 0.7181\n",
      "------------------------------\n",
      "Epoch 12/50\n",
      "Train Loss: 0.6686\n",
      "Val Loss: 0.7154\n",
      "------------------------------\n",
      "Epoch 13/50\n",
      "Train Loss: 0.6662\n",
      "Val Loss: 0.7125\n",
      "------------------------------\n",
      "Epoch 14/50\n",
      "Train Loss: 0.6639\n",
      "Val Loss: 0.7093\n",
      "------------------------------\n",
      "Epoch 15/50\n",
      "Train Loss: 0.6615\n",
      "Val Loss: 0.7058\n",
      "------------------------------\n",
      "Epoch 16/50\n",
      "Train Loss: 0.6592\n",
      "Val Loss: 0.7021\n",
      "------------------------------\n",
      "Epoch 17/50\n",
      "Train Loss: 0.6568\n",
      "Val Loss: 0.6982\n",
      "------------------------------\n",
      "Epoch 18/50\n",
      "Train Loss: 0.6545\n",
      "Val Loss: 0.6941\n",
      "------------------------------\n",
      "Epoch 19/50\n",
      "Train Loss: 0.6522\n",
      "Val Loss: 0.6898\n",
      "------------------------------\n",
      "Epoch 20/50\n",
      "Train Loss: 0.6499\n",
      "Val Loss: 0.6855\n",
      "------------------------------\n",
      "Epoch 21/50\n",
      "Train Loss: 0.6476\n",
      "Val Loss: 0.6811\n",
      "------------------------------\n",
      "Epoch 22/50\n",
      "Train Loss: 0.6452\n",
      "Val Loss: 0.6767\n",
      "------------------------------\n",
      "Epoch 23/50\n",
      "Train Loss: 0.6429\n",
      "Val Loss: 0.6723\n",
      "------------------------------\n",
      "Epoch 24/50\n",
      "Train Loss: 0.6406\n",
      "Val Loss: 0.6679\n",
      "------------------------------\n",
      "Epoch 25/50\n",
      "Train Loss: 0.6383\n",
      "Val Loss: 0.6636\n",
      "------------------------------\n",
      "Epoch 26/50\n",
      "Train Loss: 0.6360\n",
      "Val Loss: 0.6594\n",
      "------------------------------\n",
      "Epoch 27/50\n",
      "Train Loss: 0.6337\n",
      "Val Loss: 0.6553\n",
      "------------------------------\n",
      "Epoch 28/50\n",
      "Train Loss: 0.6314\n",
      "Val Loss: 0.6514\n",
      "------------------------------\n",
      "Epoch 29/50\n",
      "Train Loss: 0.6290\n",
      "Val Loss: 0.6476\n",
      "------------------------------\n",
      "Epoch 30/50\n",
      "Train Loss: 0.6267\n",
      "Val Loss: 0.6439\n",
      "------------------------------\n",
      "Epoch 31/50\n",
      "Train Loss: 0.6244\n",
      "Val Loss: 0.6404\n",
      "------------------------------\n",
      "Epoch 32/50\n",
      "Train Loss: 0.6220\n",
      "Val Loss: 0.6371\n",
      "------------------------------\n",
      "Epoch 33/50\n",
      "Train Loss: 0.6196\n",
      "Val Loss: 0.6340\n",
      "------------------------------\n",
      "Epoch 34/50\n",
      "Train Loss: 0.6173\n",
      "Val Loss: 0.6311\n",
      "------------------------------\n",
      "Epoch 35/50\n",
      "Train Loss: 0.6149\n",
      "Val Loss: 0.6283\n",
      "------------------------------\n",
      "Epoch 36/50\n",
      "Train Loss: 0.6125\n",
      "Val Loss: 0.6257\n",
      "------------------------------\n",
      "Epoch 37/50\n",
      "Train Loss: 0.6100\n",
      "Val Loss: 0.6232\n",
      "------------------------------\n",
      "Epoch 38/50\n",
      "Train Loss: 0.6076\n",
      "Val Loss: 0.6208\n",
      "------------------------------\n",
      "Epoch 39/50\n",
      "Train Loss: 0.6052\n",
      "Val Loss: 0.6185\n",
      "------------------------------\n",
      "Epoch 40/50\n",
      "Train Loss: 0.6027\n",
      "Val Loss: 0.6162\n",
      "------------------------------\n",
      "Epoch 41/50\n",
      "Train Loss: 0.6002\n",
      "Val Loss: 0.6140\n",
      "------------------------------\n",
      "Epoch 42/50\n",
      "Train Loss: 0.5977\n",
      "Val Loss: 0.6117\n",
      "------------------------------\n",
      "Epoch 43/50\n",
      "Train Loss: 0.5952\n",
      "Val Loss: 0.6094\n",
      "------------------------------\n",
      "Epoch 44/50\n",
      "Train Loss: 0.5927\n",
      "Val Loss: 0.6070\n",
      "------------------------------\n",
      "Epoch 45/50\n",
      "Train Loss: 0.5902\n",
      "Val Loss: 0.6046\n",
      "------------------------------\n",
      "Epoch 46/50\n",
      "Train Loss: 0.5876\n",
      "Val Loss: 0.6021\n",
      "------------------------------\n",
      "Epoch 47/50\n",
      "Train Loss: 0.5851\n",
      "Val Loss: 0.5995\n",
      "------------------------------\n",
      "Epoch 48/50\n",
      "Train Loss: 0.5825\n",
      "Val Loss: 0.5969\n",
      "------------------------------\n",
      "Epoch 49/50\n",
      "Train Loss: 0.5800\n",
      "Val Loss: 0.5942\n",
      "------------------------------\n",
      "Epoch 50/50\n",
      "Train Loss: 0.5774\n",
      "Val Loss: 0.5914\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import Lambda\n",
    "# Dataset and DataLoader\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    model = UNet(in_channels=3, out_channels=1)\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = ISBI2012Dataset(root='data')\n",
    "    val_dataset = ISBI2012Dataset(root='data')\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    segmenter = ImageSegmenter(model, criterion, optimizer, device, scheduler)\n",
    "    \n",
    "    # Train the model\n",
    "    segmenter.fit(train_loader, val_loader, epochs=50)\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import Compose, Resize, ToTensor\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up the device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Load the model once\n",
    "# model = UNet()\n",
    "# model.load_state_dict(torch.load('model.pth', map_location=device))\n",
    "# model.eval().to(device)\n",
    "\n",
    "# # Preprocess the image\n",
    "# transform = Compose([Resize((256, 256)), ToTensor()])\n",
    "# image = Image.open('pexels-pixabay-206959.jpg')\n",
    "# input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# # Inference\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_tensor)\n",
    "\n",
    "# # Post-process the result\n",
    "# output = torch.sigmoid(output).squeeze().cpu().numpy()  # Remove all extra dimensions\n",
    "# print(\"Output shape:\", output.shape)\n",
    "\n",
    "# # Make sure output is 2D\n",
    "# if len(output.shape) > 2:\n",
    "#     output = output.squeeze()  # Remove any extra dimensions\n",
    "#     # If it's still 3D, take the first channel\n",
    "#     if len(output.shape) > 2:\n",
    "#         output = output[0]\n",
    "\n",
    "# # Convert the predicted mask to an image\n",
    "# output_image = (output * 255).astype(np.uint8)\n",
    "# predicted_mask_image = Image.fromarray(output_image, mode='L')  # Specify mode='L' for grayscale\n",
    "\n",
    "# # Save the predicted mask image\n",
    "# predicted_mask_image.save('predicted_mask.png')\n",
    "\n",
    "# # Visualize the result with matplotlib\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Show the original image\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title('Original Image')\n",
    "# plt.imshow(image)\n",
    "# plt.axis('off')\n",
    "\n",
    "# # Show the predicted mask\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title('Predicted Mask')\n",
    "# plt.imshow(output_image, cmap='gray')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "UNet.__init__() got an unexpected keyword argument 'out_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m     visualize_results(image_path, mask)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 60\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Preprocess the input image\u001b[39;00m\n\u001b[1;32m     63\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpexels-pixabay-206959.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[79], line 10\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_path, device):\n\u001b[0;32m---> 10\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path))\n\u001b[1;32m     12\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: UNet.__init__() got an unexpected keyword argument 'out_channels'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from unet import UNet  # Your UNet model\n",
    "\n",
    "# Load the trained model\n",
    "def load_model(model_path, device):\n",
    "    model = UNet()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Preprocess input image (resize, normalize, etc.)\n",
    "def preprocess_image(image_path, device):\n",
    "    image = Image.open(image_path).convert('RGB')  # Open the image and convert to RGB\n",
    "    transform = T.Compose([\n",
    "        T.Resize((256, 256)),  # Resize to match model input size\n",
    "        T.ToTensor(),  # Convert to tensor\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image.to(device)\n",
    "\n",
    "# Run the model on the input image and get the output mask\n",
    "def predict_segmentation(model, image, device):\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        # Apply sigmoid to get probabilities\n",
    "        output = torch.sigmoid(output)\n",
    "        # Convert the output to a binary mask (thresholding at 0.5)\n",
    "        mask = output.squeeze().cpu().numpy() > 0.5\n",
    "    return mask\n",
    "\n",
    "# Visualize the image and the predicted mask\n",
    "def visualize_results(image_path, mask):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = np.array(image)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    # Predicted mask\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Predicted Mask')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model('best_model.pth', device)\n",
    "\n",
    "    # Preprocess the input image\n",
    "    image_path = 'pexels-pixabay-206959.jpg'  # Replace with your image path\n",
    "    image = preprocess_image(image_path, device)\n",
    "\n",
    "    # Get the segmentation mask prediction\n",
    "    mask = predict_segmentation(model, image, device)\n",
    "\n",
    "    # Visualize the results\n",
    "    visualize_results(image_path, mask)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
