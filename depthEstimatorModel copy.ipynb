{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aab7c48",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aariziqbal/Desktop/Homework/ECS_174/project/CVFinalProject/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4367a64d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 1. Load MiDaS Model for Depth Estimation\n",
    "def load_midas_model():\n",
    "    \"\"\"\n",
    "    Loads the MiDaS model for depth estimation.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): The loaded MiDaS model.\n",
    "        transform (torchvision.transforms.Compose): The transformation applied to input images.\n",
    "    \"\"\"\n",
    "    model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "    model.eval()\n",
    "    transform = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").small_transform\n",
    "    return model, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed6a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet is a convolutional neural network architecture for image segmentation.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels. Default is 3.\n",
    "        out_channels (int): Number of output channels. Default is 1.\n",
    "    \n",
    "    Attributes:\n",
    "        encoder (nn.Module): Encoder part of the UNet model.\n",
    "        decoder (nn.Module): Decoder part of the UNet model.\n",
    "    \n",
    "    Methods:\n",
    "        forward(x): Forward pass of the UNet model.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = timm.create_model(\"resnet34\", features_only=True, pretrained=True)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the UNet model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_channels, height, width).\n",
    "        \"\"\"\n",
    "        enc_outs = self.encoder(x)\n",
    "        out = self.decoder(enc_outs[-1])\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a6e8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unet_model():\n",
    "    \"\"\"\n",
    "    Load the pre-trained UNet model for depth estimation.\n",
    "\n",
    "    Returns:\n",
    "        model (UNet): The loaded UNet model.\n",
    "    \"\"\"\n",
    "    model = UNet()\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(\"path_to_pretrained_unet.pth\", map_location='cpu'))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pre-trained U-Net model not found. Using untrained model.\")\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57299e78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def estimate_depth(model, transform, image):\n",
    "    \"\"\"\n",
    "    Estimates the depth of an image using a given model and transformation.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The depth estimation model.\n",
    "    transform (torchvision.transforms.Compose): The transformation to be applied to the image.\n",
    "    image (PIL.Image.Image): The input image.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The estimated depth map.\n",
    "    \"\"\"\n",
    "    input_batch = transform(image)  # Add batch dimension\n",
    "    print(\"Input Batch Shape\",input_batch.shape)\n",
    "    with torch.no_grad():\n",
    "        depth = model(input_batch)\n",
    "        print(\"Depth Shape\",depth.shape)\n",
    "        # Fix: Ensure depth has the correct shape [batch_size, channels, height, width]\n",
    "        if len(depth.shape) == 5:\n",
    "            \n",
    "            depth = depth.squeeze(1)  # Remove the extra dimension\n",
    "            \n",
    "        depth = depth.squeeze().cpu().numpy()\n",
    "        depth = cv2.resize(depth, (image.shape[1], image.shape[0]))\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d268eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_food(unet_model, image):\n",
    "    \"\"\"\n",
    "    Segment food in an image using a U-Net model.\n",
    "\n",
    "    Parameters:\n",
    "    - unet_model (torch.nn.Module): The U-Net model used for segmentation.\n",
    "    - image (PIL.Image.Image): The input image to be segmented.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The binary mask indicating the segmented food regions.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((256, 256)),\n",
    "    ])\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        mask = unet_model(input_tensor)\n",
    "        mask = mask.squeeze().cpu().numpy()\n",
    "        mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
    "        return mask > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df2bf7c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_volume(depth_map, mask, pixel_area=0.1):\n",
    "    volume = np.sum(depth_map[mask]) * pixel_area\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787d7c73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to perform depth estimation and volume calculation for a given image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The path to the input image file.\n",
    "    \"\"\"\n",
    "    # Load Image\n",
    "    image_path = 'pexels-pixabay-206959.jpg'\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    midas_model, midas_transform = load_midas_model()\n",
    "    unet_model = load_unet_model()\n",
    "    \n",
    "    food_mask = segment_food(unet_model, image)\n",
    "    \n",
    "    depth_map = estimate_depth(midas_model, midas_transform, image)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Segmented Food\")\n",
    "    plt.imshow(food_mask, cmap='gray')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Depth Map\")\n",
    "    plt.imshow(depth_map, cmap='viridis')\n",
    "    plt.show()\n",
    "    \n",
    "    volume = calculate_volume(depth_map, food_mask)\n",
    "    print(f\"Estimated Volume: {volume:.2f} cubic units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5ff065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@0.269] global loadsave.cpp:241 findDecoder imread_('pexels-pixabay-206959.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpexels-pixabay-206959.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 11\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m midas_model, midas_transform \u001b[38;5;241m=\u001b[39m load_midas_model()\n\u001b[1;32m     14\u001b[0m unet_model \u001b[38;5;241m=\u001b[39m load_unet_model()\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
